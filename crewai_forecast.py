# CrewAI + Gemini API éŠ·å”®é æ¸¬ç³»çµ± (çœŸå¯¦æ•¸æ“šç‰ˆ)
# ä½¿ç”¨ sales_cube.db ä¸­çš„çœŸå¯¦éŠ·å”®æ•¸æ“š

import os
from dotenv import load_dotenv
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
import warnings
import sqlite3
from statsmodels.tsa.statespace.sarimax import SARIMAX
import requests
import json
from crewai import Crew, Agent, Task
from langchain_core.tools import Tool
from langchain_google_genai import ChatGoogleGenerativeAI
warnings.filterwarnings('ignore')

# === 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸ ===
load_dotenv()
API_KEY = os.getenv("GOOGLE_API_KEY")

# æª¢æŸ¥ API Key æ˜¯å¦å­˜åœ¨
if not API_KEY or API_KEY == "your_gemini_api_key_here":
    print("âŒ éŒ¯èª¤ï¼šè«‹è¨­ç½® GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸")
    print("è«‹åœ¨ .env æª”æ¡ˆä¸­æ·»åŠ ï¼šGOOGLE_API_KEY=ä½ çš„Gemini API Key")
    exit(1)

# === 2. é…ç½® Gemini LLM ===
def create_gemini_llm():
    """å‰µå»º Gemini LLM å¯¦ä¾‹"""
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-pro",
            google_api_key=API_KEY,
            temperature=0.7,
            max_tokens=2048
        )
        return llm
    except ImportError:
        print("âŒ éŒ¯èª¤ï¼šéœ€è¦å®‰è£ langchain-google-genai")
        print("è«‹åŸ·è¡Œï¼špip install langchain-google-genai")
        exit(1)
    except Exception as e:
        print(f"âŒ Gemini LLM å‰µå»ºå¤±æ•—ï¼š{e}")
        exit(1)

# === 3. è³‡æ–™åº«åˆ†æå·¥å…· ===
class DatabaseAnalysisTools:
    """è³‡æ–™åº«åˆ†æå·¥å…·é¡"""
    
    def __init__(self, db_path='sales_cube.db'):
        self.db_path = db_path
    
    def get_database_info(self):
        """ç²å–è³‡æ–™åº«åŸºæœ¬ä¿¡æ¯"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ç²å–è¡¨ä¿¡æ¯
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            # ç²å–å„è¡¨çš„è¨˜éŒ„æ•¸
            table_counts = {}
            for table in tables:
                table_name = table[0]
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                count = cursor.fetchone()[0]
                table_counts[table_name] = count
            
            conn.close()
            
            return {
                'success': True,
                'tables': [table[0] for table in tables],
                'table_counts': table_counts,
                'database_path': self.db_path
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_sales_summary(self):
        """ç²å–éŠ·å”®æ•¸æ“šæ‘˜è¦"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # æœˆåº¦éŠ·å”®æ‘˜è¦
            monthly_query = """
            SELECT 
                substr(dt.date, 1, 7) as month,
                SUM(sf.amount) as total_sales,
                COUNT(sf.sale_id) as transaction_count,
                AVG(sf.amount) as avg_sale_amount
            FROM sales_fact sf
            JOIN dim_time dt ON sf.time_id = dt.time_id
            GROUP BY substr(dt.date, 1, 7)
            ORDER BY month
            """
            monthly_df = pd.read_sql_query(monthly_query, conn)
            
            # ç”¢å“é¡åˆ¥æ‘˜è¦
            product_query = """
            SELECT 
                dp.category,
                SUM(sf.amount) as category_sales,
                COUNT(sf.sale_id) as category_transactions
            FROM sales_fact sf
            JOIN dim_product dp ON sf.product_id = dp.product_id
            GROUP BY dp.category
            ORDER BY category_sales DESC
            """
            product_df = pd.read_sql_query(product_query, conn)
            
            # åœ°å€æ‘˜è¦
            region_query = """
            SELECT 
                dr.region_name,
                SUM(sf.amount) as region_sales,
                COUNT(sf.sale_id) as region_transactions
            FROM sales_fact sf
            JOIN dim_region dr ON sf.region_id = dr.region_id
            GROUP BY dr.region_name
            ORDER BY region_sales DESC
            """
            region_df = pd.read_sql_query(region_query, conn)
            
            # æ™‚é–“ç¯„åœ
            time_query = """
            SELECT MIN(date), MAX(date) FROM dim_time
            """
            time_range = pd.read_sql_query(time_query, conn)
            
            conn.close()
            
            return {
                'success': True,
                'monthly_data': monthly_df.to_dict('records'),
                'product_data': product_df.to_dict('records'),
                'region_data': region_df.to_dict('records'),
                'time_range': {
                    'start_date': time_range.iloc[0, 0],
                    'end_date': time_range.iloc[0, 1]
                },
                'total_records': len(monthly_df),
                'total_sales': monthly_df['total_sales'].sum(),
                'total_transactions': monthly_df['transaction_count'].sum()
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_data_quality_report(self):
        """ç²å–æ•¸æ“šå“è³ªå ±å‘Š"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # æª¢æŸ¥ç¼ºå¤±å€¼
            missing_check = """
            SELECT 
                'sales_fact' as table_name,
                COUNT(*) as total_rows,
                SUM(CASE WHEN amount IS NULL THEN 1 ELSE 0 END) as null_amount,
                SUM(CASE WHEN sale_id IS NULL THEN 1 ELSE 0 END) as null_sale_id
            FROM sales_fact
            """
            missing_df = pd.read_sql_query(missing_check, conn)
            
            # æª¢æŸ¥ç•°å¸¸å€¼
            outlier_check = """
            SELECT 
                MIN(amount) as min_amount,
                MAX(amount) as max_amount,
                AVG(amount) as avg_amount,
                STDDEV(amount) as std_amount
            FROM sales_fact
            """
            outlier_df = pd.read_sql_query(outlier_check, conn)
            
            # æª¢æŸ¥æ™‚é–“åºåˆ—é€£çºŒæ€§
            time_continuity = """
            SELECT 
                COUNT(DISTINCT date) as unique_dates,
                MIN(date) as min_date,
                MAX(date) as max_date
            FROM dim_time
            """
            continuity_df = pd.read_sql_query(time_continuity, conn)
            
            conn.close()
            
            return {
                'success': True,
                'missing_data': missing_df.to_dict('records'),
                'outlier_stats': outlier_df.to_dict('records'),
                'time_continuity': continuity_df.to_dict('records')
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

# === 4. å‰µå»ºå·¥å…·å¯¦ä¾‹ ===
db_tools = DatabaseAnalysisTools()

# === 5. çœŸå¯¦æ•¸æ“šé æ¸¬å·¥å…· ===
class RealDataForecastTools:
    """ä½¿ç”¨çœŸå¯¦æ•¸æ“šçš„é æ¸¬å·¥å…·"""
    
    def __init__(self, db_path='sales_cube.db'):
        self.db_path = db_path
        
    def get_sales_data(self):
        """ç²å–çœŸå¯¦éŠ·å”®æ•¸æ“š"""
        try:
            conn = sqlite3.connect(self.db_path)
            
            # ç²å–æœˆåº¦éŠ·å”®æ•¸æ“š
            query = """
            SELECT 
                substr(dt.date, 1, 7) as month,
                SUM(sf.amount) as total_sales,
                COUNT(sf.sale_id) as transaction_count,
                AVG(sf.amount) as avg_sale_amount
            FROM sales_fact sf
            JOIN dim_time dt ON sf.time_id = dt.time_id
            GROUP BY substr(dt.date, 1, 7)
            ORDER BY month
            """
            df = pd.read_sql_query(query, conn)
            
            # ç²å–ç”¢å“é¡åˆ¥éŠ·å”®æ•¸æ“š
            product_query = """
            SELECT 
                dp.category,
                SUM(sf.amount) as category_sales,
                COUNT(sf.sale_id) as category_transactions
            FROM sales_fact sf
            JOIN dim_product dp ON sf.product_id = dp.product_id
            GROUP BY dp.category
            ORDER BY category_sales DESC
            """
            product_df = pd.read_sql_query(product_query, conn)
            
            # ç²å–åœ°å€éŠ·å”®æ•¸æ“š
            region_query = """
            SELECT 
                dr.region_name,
                SUM(sf.amount) as region_sales,
                COUNT(sf.sale_id) as region_transactions
            FROM sales_fact sf
            JOIN dim_region dr ON sf.region_id = dr.region_id
            GROUP BY dr.region_name
            ORDER BY region_sales DESC
            """
            region_df = pd.read_sql_query(region_query, conn)
            
            conn.close()
            
            if df.empty:
                print("âš ï¸  è³‡æ–™åº«ä¸­æ²’æœ‰éŠ·å”®æ•¸æ“šï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
                return self.generate_sample_data()
            
            print(f"âœ… æˆåŠŸå¾è³‡æ–™åº«ç²å– {len(df)} å€‹æœˆçš„çœŸå¯¦éŠ·å”®æ•¸æ“š")
            print(f"ğŸ“Š æ•¸æ“šç¯„åœï¼š{df['month'].min()} åˆ° {df['month'].max()}")
            print(f"ğŸ’° ç¸½éŠ·å”®é¡ï¼š{df['total_sales'].sum():,.2f} å…ƒ")
            print(f"ğŸ›’ ç¸½äº¤æ˜“æ•¸ï¼š{df['transaction_count'].sum():,} ç­†")
            
            return {
                'monthly_data': df,
                'product_data': product_df,
                'region_data': region_df,
                'dates': df['month'].tolist(),
                'sales': df['total_sales'].tolist()
            }
            
        except Exception as e:
            print(f"âŒ ç„¡æ³•é€£æ¥è³‡æ–™åº«ï¼š{e}")
            print("ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šä½œç‚ºå‚™ç”¨")
            return self.generate_sample_data()
    
    def generate_sample_data(self):
        """ç”Ÿæˆç¤ºä¾‹éŠ·å”®æ•¸æ“šï¼ˆå‚™ç”¨ï¼‰"""
        dates = []
        sales_data = []
        
        # ä½¿ç”¨å›ºå®šæ—¥æœŸä½œç‚ºåŸºæº–ï¼Œç¢ºä¿æ™‚é–“è»¸ä¸€è‡´æ€§
        base_date = datetime(2025, 7, 10)  # èˆ‡å…¶ä»–æ¨¡çµ„ä¿æŒä¸€è‡´
        for i in range(24):
            date = base_date - timedelta(days=30*(23-i))
            dates.append(date.strftime("%Y-%m"))
            
            base_sales = 100000
            trend = i * 5000
            seasonal = 20000 * np.sin(2 * np.pi * i / 12)
            noise = np.random.normal(0, 10000)
            
            sales = base_sales + trend + seasonal + noise
            sales_data.append(max(0, sales))
        
        return {
            'dates': dates,
            'sales': sales_data,
            'monthly_data': pd.DataFrame({'month': dates, 'total_sales': sales_data})
        }
    
    def forecast_sales(self, periods=12):
        """åŸ·è¡ŒéŠ·å”®é æ¸¬"""
        try:
            data = self.get_sales_data()
            dates = data['dates']
            sales_data = data['sales']
            
            if len(dates) < 3:
                print("âŒ æ•¸æ“šé»ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
                return {
                    'success': False,
                    'error': 'æ•¸æ“šé»ä¸è¶³ï¼Œè‡³å°‘éœ€è¦3å€‹æ•¸æ“šé»'
                }
            
            historical_series = pd.Series(sales_data, index=pd.to_datetime(dates))
            
            # ä½¿ç”¨ SARIMAX æ¨¡å‹
            model = SARIMAX(historical_series,
                          order=(1, 1, 1),
                          seasonal_order=(1, 1, 1, 12),
                          enforce_stationarity=False,
                          enforce_invertibility=False)
            
            results = model.fit(disp=False)
            forecast = results.forecast(steps=periods)
            
            # ç”Ÿæˆé æ¸¬æ—¥æœŸ - ä½¿ç”¨å›ºå®šåŸºæº–æ—¥æœŸç¢ºä¿ä¸€è‡´æ€§
            base_date = datetime(2025, 7, 10)  # èˆ‡å…¶ä»–æ¨¡çµ„ä¿æŒä¸€è‡´
            forecast_dates = []
            
            # å¾2025å¹´8æœˆé–‹å§‹é æ¸¬
            start_year = 2025
            start_month = 8
            
            for i in range(periods):
                if start_month > 12:
                    start_month = 1
                    start_year += 1
                forecast_dates.append(f"{start_year}-{start_month:02d}")
                start_month += 1
            
            # è™•ç†é æ¸¬çµæœ
            forecast_data = []
            for i, (date, value) in enumerate(zip(forecast_dates, forecast.values)):
                forecast_data.append({
                    'period': date,
                    'forecast_sales': round(float(value), 2),
                    'period_number': i + 1
                })
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            total_forecast = sum(item['forecast_sales'] for item in forecast_data)
            avg_forecast = total_forecast / len(forecast_data)
            
            # è¨ˆç®—æ­·å²çµ±è¨ˆä¿¡æ¯
            historical_stats = {
                'total_sales': sum(sales_data),
                'avg_monthly_sales': np.mean(sales_data),
                'sales_std': np.std(sales_data),
                'min_sales': min(sales_data),
                'max_sales': max(sales_data),
                'data_points': len(sales_data)
            }
            
            return {
                'success': True,
                'forecast_data': forecast_data,
                'periods': periods,
                'total_forecast': total_forecast,
                'avg_forecast': avg_forecast,
                'historical_data': {
                    'dates': dates,
                    'sales': sales_data,
                    'stats': historical_stats
                },
                'raw_data': data
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

# === 6. å‰µå»ºå·¥å…·å¯¦ä¾‹ ===
forecast_tools = RealDataForecastTools()

# === 7. å‰µå»º Gemini LLM ===
gemini_llm = create_gemini_llm()

# === 8. å®šç¾©å·¥å…·å‡½æ•¸ ===
def analyze_database_tool(tool_input):
    """åˆ†æè³‡æ–™åº«å·¥å…·"""
    db_info = db_tools.get_database_info()
    sales_summary = db_tools.get_sales_summary()
    quality_report = db_tools.get_data_quality_report()
    
    return {
        'database_info': db_info,
        'sales_summary': sales_summary,
        'quality_report': quality_report
    }

def get_forecast_data_tool(tool_input):
    """ç²å–é æ¸¬æ•¸æ“šå·¥å…·"""
    # å¾ tool_input ä¸­æå– periods åƒæ•¸ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é è¨­å€¼
    periods = 12
    if isinstance(tool_input, dict) and 'periods' in tool_input:
        periods = tool_input['periods']
    elif isinstance(tool_input, str):
        try:
            # å˜—è©¦å¾å­—ç¬¦ä¸²ä¸­æå–æ•¸å­—
            import re
            numbers = re.findall(r'\d+', tool_input)
            if numbers:
                periods = int(numbers[0])
        except:
            pass
    
    return forecast_tools.forecast_sales(periods)

# === 9. å‰µå»ºå·¥å…· ===
database_tool = Tool(
    name="analyze_database",
    description="åˆ†æ sales_cube.db è³‡æ–™åº«ä¸­çš„çœŸå¯¦éŠ·å”®æ•¸æ“šï¼ŒåŒ…æ‹¬æ•¸æ“šæ‘˜è¦ã€å“è³ªå ±å‘Šå’Œçµ±è¨ˆä¿¡æ¯",
    func=analyze_database_tool
)

forecast_tool = Tool(
    name="get_forecast_data",
    description="ä½¿ç”¨çœŸå¯¦éŠ·å”®æ•¸æ“šåŸ·è¡Œ SARIMAX é æ¸¬åˆ†æ",
    func=get_forecast_data_tool
)

# === 10. å®šç¾© Agents ===
data_agent = Agent(
    role="è³‡æ–™å·¥ç¨‹å¸«",
    goal="åˆ†æçœŸå¯¦éŠ·å”®æ•¸æ“šä¸¦æä¾›æ•¸æ“šå“è³ªå ±å‘Š",
    backstory="å°ˆæ¥­çš„è³‡æ–™å·¥ç¨‹å¸«ï¼Œæ“…é•·è³‡æ–™æ¸…ç†å’Œé è™•ç†ã€‚æˆ‘æœƒåˆ†æçœŸå¯¦éŠ·å”®æ•¸æ“šçš„å“è³ªä¸¦æä¾›è©³ç´°çš„æ•¸æ“šæ‘˜è¦ã€‚",
    llm=gemini_llm,
    tools=[database_tool],
    verbose=True
)

forecast_agent = Agent(
    role="é æ¸¬åˆ†æå¸«",
    goal="ä½¿ç”¨çœŸå¯¦æ•¸æ“šåŸ·è¡ŒéŠ·å”®é æ¸¬ä¸¦åˆ†æçµæœ",
    backstory="è³‡æ·±é æ¸¬åˆ†æå¸«ï¼Œç²¾é€šæ™‚é–“åºåˆ—åˆ†æã€‚æˆ‘æœƒä½¿ç”¨çœŸå¯¦éŠ·å”®æ•¸æ“šæ§‹å»º SARIMAX æ¨¡å‹ä¸¦æä¾›çµ±è¨ˆåˆ†æã€‚",
    llm=gemini_llm,
    tools=[forecast_tool],
    verbose=True
)

analysis_agent = Agent(
    role="æ¥­å‹™åˆ†æå¸«",
    goal="åŸºæ–¼çœŸå¯¦æ•¸æ“šåˆ†æé æ¸¬çµæœä¸¦æä¾›æ¥­å‹™æ´å¯Ÿ",
    backstory="è³‡æ·±æ¥­å‹™åˆ†æå¸«ï¼Œæ“…é•·å¸‚å ´åˆ†æå’Œç­–ç•¥è¦åŠƒã€‚æˆ‘æœƒåŸºæ–¼çœŸå¯¦éŠ·å”®æ•¸æ“šæä¾›è¶¨å‹¢åˆ†æã€é¢¨éšªè©•ä¼°å’Œç­–ç•¥å»ºè­°ã€‚",
    llm=gemini_llm,
    verbose=True
)

# === 11. å®šç¾© Tasks ===
data_task = Task(
    description="""
    ä½œç‚ºè³‡æ–™å·¥ç¨‹å¸«ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
    
    1. ä½¿ç”¨ analyze_database å·¥å…·åˆ†æ sales_cube.db ä¸­çš„çœŸå¯¦éŠ·å”®æ•¸æ“š
    2. æª¢æŸ¥æ•¸æ“šå“è³ªï¼ŒåŒ…æ‹¬ï¼š
       - æ•¸æ“šå®Œæ•´æ€§ï¼ˆç¼ºå¤±å€¼ã€é‡è¤‡å€¼ï¼‰
       - ç•°å¸¸å€¼æª¢æ¸¬
       - æ•¸æ“šæ ¼å¼æª¢æŸ¥
       - æ™‚é–“åºåˆ—çš„é€£çºŒæ€§
    3. æä¾›æ•¸æ“šæ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š
       - æ•¸æ“šé»æ•¸é‡å’Œæ™‚é–“ç¯„åœ
       - éŠ·å”®é¡çµ±è¨ˆï¼ˆç¸½é¡ã€å¹³å‡å€¼ã€æ¨™æº–å·®ã€æœ€å¤§å€¼ã€æœ€å°å€¼ï¼‰
       - äº¤æ˜“æ•¸é‡çµ±è¨ˆ
       - ç”¢å“é¡åˆ¥åˆ†æ
       - åœ°å€éŠ·å”®åˆ†æ
    4. è­˜åˆ¥æ•¸æ“šä¸­çš„æ¨¡å¼å’Œè¶¨å‹¢
    
    è«‹æä¾›è©³ç´°çš„çœŸå¯¦æ•¸æ“šåˆ†æå ±å‘Šã€‚
    """,
    expected_output="å®Œæ•´çš„çœŸå¯¦éŠ·å”®æ•¸æ“šåˆ†æå ±å‘Šå’Œå“è³ªè©•ä¼°",
    agent=data_agent
)

forecast_task = Task(
    description="""
    ä½œç‚ºé æ¸¬åˆ†æå¸«ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
    
    1. æ¥æ”¶è³‡æ–™å·¥ç¨‹å¸«æä¾›çš„çœŸå¯¦éŠ·å”®æ•¸æ“šåˆ†æ
    2. ä½¿ç”¨ get_forecast_data å·¥å…·é€²è¡ŒéŠ·å”®é æ¸¬ï¼ŒåŒ…æ‹¬ï¼š
       - æ¨¡å‹åƒæ•¸é¸æ“‡ (p, d, q) å’Œå­£ç¯€æ€§åƒæ•¸ (P, D, Q, s)
       - æ¨¡å‹æ“¬åˆå’Œè¨ºæ–·
       - é æ¸¬æº–ç¢ºæ€§è©•ä¼°
    3. åˆ†æé æ¸¬çµæœï¼ŒåŒ…æ‹¬ï¼š
       - é æ¸¬å€¼çµ±è¨ˆæ‘˜è¦
       - ç½®ä¿¡å€é–“åˆ†æ
       - è¶¨å‹¢åˆ†æ
       - å­£ç¯€æ€§æ¨¡å¼è­˜åˆ¥
    4. æä¾›è©³ç´°çš„é æ¸¬å ±å‘Š
    
    è«‹æä¾›å®Œæ•´çš„é æ¸¬çµæœå’Œçµ±è¨ˆåˆ†æã€‚
    """,
    expected_output="è©³ç´°çš„é æ¸¬çµæœå’Œçµ±è¨ˆåˆ†æå ±å‘Š",
    agent=forecast_agent,
    context=[data_task]
)

analysis_task = Task(
    description="""
    ä½œç‚ºæ¥­å‹™åˆ†æå¸«ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
    
    1. æ¥æ”¶é æ¸¬åˆ†æå¸«çš„é æ¸¬çµæœ
    2. åŸºæ–¼çœŸå¯¦æ•¸æ“šé€²è¡Œæ¥­å‹™åˆ†æï¼ŒåŒ…æ‹¬ï¼š
       - éŠ·å”®è¶¨å‹¢åˆ†æï¼ˆåŸºæ–¼çœŸå¯¦æ­·å²æ•¸æ“šï¼‰
       - å¸‚å ´æ©Ÿæœƒè©•ä¼°
       - é¢¨éšªå› ç´ è­˜åˆ¥
       - ç«¶çˆ­ç’°å¢ƒåˆ†æ
    3. æä¾›ç­–ç•¥å»ºè­°ï¼ŒåŒ…æ‹¬ï¼š
       - è³‡æºé…ç½®å»ºè­°ï¼ˆåŸºæ–¼çœŸå¯¦éŠ·å”®è¡¨ç¾ï¼‰
       - ç‡ŸéŠ·ç­–ç•¥å»ºè­°
       - é¢¨éšªç®¡ç†å»ºè­°
       - ç¸¾æ•ˆç›£æ§å»ºè­°
    
    è«‹æä¾›å®Œæ•´çš„æ¥­å‹™åˆ†æå ±å‘Šå’Œç­–ç•¥å»ºè­°ã€‚
    """,
    expected_output="å®Œæ•´çš„æ¥­å‹™åˆ†æå ±å‘Šå’Œç­–ç•¥å»ºè­°",
    agent=analysis_agent,
    context=[forecast_task]
)

# === 12. çµ„è£ Crew ===
crew = Crew(
    agents=[data_agent, forecast_agent, analysis_agent],
    tasks=[data_task, forecast_task, analysis_task],
    verbose=True
)

# === 13. åŸ·è¡Œå‡½æ•¸ ===
def execute_crewai_forecast(periods=12):
    """åŸ·è¡Œ CrewAI é æ¸¬æµç¨‹"""
    try:
        print("ğŸš€ é–‹å§‹åŸ·è¡Œ CrewAI + Gemini çœŸå¯¦æ•¸æ“šé æ¸¬æµç¨‹...")
        print(f"ğŸ“Š é æ¸¬æœŸæ•¸ï¼š{periods}")
        print(f"ğŸ”‘ ä½¿ç”¨ Gemini API Key: {API_KEY[:10]}...")
        
        # å…ˆåŸ·è¡Œé æ¸¬ç²å–æ•¸æ“š
        print("ğŸ“‹ å¾ sales_cube.db ç²å–çœŸå¯¦éŠ·å”®æ•¸æ“š...")
        forecast_result = forecast_tools.forecast_sales(periods)
        
        if not forecast_result['success']:
            print(f"âŒ é æ¸¬å¤±æ•—ï¼š{forecast_result['error']}")
            return {
                'success': False,
                'error': forecast_result['error'],
                'timestamp': datetime.now().isoformat()
            }
        
        print("âœ… çœŸå¯¦æ•¸æ“šé æ¸¬å®Œæˆ")
        print(f"ğŸ“Š é æ¸¬æ•¸æ“šï¼š{len(forecast_result['forecast_data'])} å€‹æœŸé–“")
        print(f"ğŸ’° ç¸½é æ¸¬é¡ï¼š{forecast_result['total_forecast']:,.0f} å…ƒ")
        print(f"ğŸ“ˆ å¹³å‡æœˆé æ¸¬ï¼š{forecast_result['avg_forecast']:,.0f} å…ƒ")
        
        # é¡¯ç¤ºæ­·å²æ•¸æ“šçµ±è¨ˆ
        hist_stats = forecast_result['historical_data']['stats']
        print(f"ğŸ“Š æ­·å²æ•¸æ“šçµ±è¨ˆï¼š")
        print(f"   - æ•¸æ“šé»æ•¸ï¼š{hist_stats['data_points']} å€‹æœˆ")
        print(f"   - ç¸½éŠ·å”®é¡ï¼š{hist_stats['total_sales']:,.0f} å…ƒ")
        print(f"   - å¹³å‡æœˆéŠ·å”®ï¼š{hist_stats['avg_monthly_sales']:,.0f} å…ƒ")
        print(f"   - éŠ·å”®æ¨™æº–å·®ï¼š{hist_stats['sales_std']:,.0f} å…ƒ")
        
        # åŸ·è¡Œ CrewAI æµç¨‹
        print("ğŸ¤– é–‹å§‹ CrewAI åˆ†ææµç¨‹...")
        result = crew.kickoff()
        
        print("âœ… CrewAI é æ¸¬æµç¨‹å®Œæˆ")
        return {
            'success': True,
            'crewai_result': result,
            'forecast_data': forecast_result,
            'periods': periods,
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ CrewAI é æ¸¬æµç¨‹å¤±æ•—ï¼š{str(e)}")
        return {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

# === 14. ä¸»ç¨‹å¼åŸ·è¡Œ ===
if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ CrewAI + Gemini çœŸå¯¦æ•¸æ“šéŠ·å”®é æ¸¬ç³»çµ±...")
    print(f"âœ… ä½¿ç”¨ Gemini API Key: {API_KEY[:10]}...")
    print("ğŸ“Š ä½¿ç”¨ sales_cube.db ä¸­çš„çœŸå¯¦éŠ·å”®æ•¸æ“š")
    
    try:
        # åŸ·è¡Œ CrewAI é æ¸¬
        result = execute_crewai_forecast(periods=12)
        
        print("=" * 60)
        print("CrewAI + Gemini çœŸå¯¦æ•¸æ“šé æ¸¬çµæœï¼š")
        print("=" * 60)
        
        if result['success']:
            print("âœ… é æ¸¬æˆåŠŸå®Œæˆ")
            print(f"ğŸ“… é æ¸¬æœŸæ•¸ï¼š{result['periods']}")
            print(f"â° åŸ·è¡Œæ™‚é–“ï¼š{result['timestamp']}")
            
            # é¡¯ç¤ºé æ¸¬æ•¸æ“šæ‘˜è¦
            forecast_data = result['forecast_data']
            print(f"ğŸ“Š é æ¸¬æ•¸æ“šé»æ•¸ï¼š{len(forecast_data['forecast_data'])}")
            print(f"ğŸ’° ç¸½é æ¸¬éŠ·å”®é¡ï¼š{forecast_data['total_forecast']:,.0f} å…ƒ")
            print(f"ğŸ“ˆ å¹³å‡æœˆéŠ·å”®é¡ï¼š{forecast_data['avg_forecast']:,.0f} å…ƒ")
            
            # é¡¯ç¤ºæ­·å²æ•¸æ“šæ‘˜è¦
            hist_data = forecast_data['historical_data']
            print(f"ğŸ“Š æ­·å²æ•¸æ“šæ‘˜è¦ï¼š")
            print(f"   - æ•¸æ“šç¯„åœï¼š{hist_data['dates'][0]} åˆ° {hist_data['dates'][-1]}")
            print(f"   - æ•¸æ“šé»æ•¸ï¼š{hist_data['stats']['data_points']} å€‹æœˆ")
            print(f"   - ç¸½æ­·å²éŠ·å”®ï¼š{hist_data['stats']['total_sales']:,.0f} å…ƒ")
            
            print("\nğŸ“‹ CrewAI åˆ†æå ±å‘Šï¼š")
            print("-" * 40)
            print(result['crewai_result'])
        else:
            print(f"âŒ é æ¸¬å¤±æ•—ï¼š{result['error']}")
        
        print("=" * 60)
        print("âœ… CrewAI + Gemini çœŸå¯¦æ•¸æ“šéŠ·å”®é æ¸¬ç³»çµ±åŸ·è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ç³»çµ±åŸ·è¡Œå¤±æ•—ï¼š{e}")
        print("è«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºï¼Œä»¥åŠç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸")

# === 15. å®‰è£èªªæ˜ ===
"""
å¦‚æœé‡åˆ°ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œï¼Œè«‹åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£å…¼å®¹ç‰ˆæœ¬ï¼š

pip uninstall crewai langchain langchain-core langchain-community
pip install crewai==0.11.0
pip install langchain==0.0.350
pip install langchain-core==0.0.12
pip install langchain-community==0.0.10
pip install langchain-google-genai==0.0.6
pip install python-dotenv
pip install pandas numpy statsmodels

ç„¶å¾Œç¢ºä¿ .env æª”æ¡ˆä¸­æœ‰æ­£ç¢ºçš„ GOOGLE_API_KEY
""" 